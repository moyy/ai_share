- [AI: 2023.10](#ai-202310)
- [1. 工具](#1-工具)
- [2. 项目 / 框架](#2-项目--框架)
  - [2.01. PDFTriage：解决上下文过长的问题](#201-pdftriage解决上下文过长的问题)
  - [2.02. 使用GPT-4 Advanced Data Analysis（原Code Interpreter）功能驱动的交互式模拟人生游戏](#202-使用gpt-4-advanced-data-analysis原code-interpreter功能驱动的交互式模拟人生游戏)
  - [2.03. Octogen：为开发者打造的开源代码解释器](#203-octogen为开发者打造的开源代码解释器)
  - [2.04. llmchain: Rust + LLM](#204-llmchain-rust--llm)
  - [2.05. Lace：用 Rust 写的概率交叉分类引擎，带有可选的 Python 接口](#205-lace用-rust-写的概率交叉分类引擎带有可选的-python-接口)
- [3. 模型](#3-模型)
  - [3.01. LLaMA 2 Long: 与GPT-4持平，上下文长度达3.2万token](#301-llama-2-long-与gpt-4持平上下文长度达32万token)
- [4. 技巧 / 教程](#4-技巧--教程)
  - [4.01. 思维链推理相关文献列表](#401-思维链推理相关文献列表)
  - [4.02. 提升 ChatGPT 翻译质量的 Prompt](#402-提升-chatgpt-翻译质量的-prompt)
  - [4.03. 让ChatGPT 校对翻译结果，检查有误遗漏呢？](#403-让chatgpt-校对翻译结果检查有误遗漏呢)
- [5. 资讯 / 观点](#5-资讯--观点)
  - [5.01. Anthropic/Claude 全面开放 API，与 GPT 的定价比较：](#501-anthropicclaude-全面开放-api与-gpt-的定价比较)

# AI: 2023.10

# 1. 工具

# 2. 项目 / 框架

## 2.01. [PDFTriage：解决上下文过长的问题](https://arxiv.org/abs/2309.08872) 

PDFTriage通过先了解文档的结构，然后精准地找到与用户问题相关的部分，最后用语言模型生成答案，从而解决了传统模型在处理长篇和复杂结构文档时的不足。

大型语言模型（LLM）在处理长篇、结构复杂的文档时面临以下几个主要问题：

+ 上下文窗口限制：LLM通常有一个固定的上下文窗口大小，这意味着它一次只能处理有限数量的文本“令牌”（tokens）。对于长篇文档，这就需要进行预处理或分割，以便模型能够处理。
+ 文档结构忽略：传统的LLM通常只处理纯文本，忽略了文档的结构信息（如页面、表格、标题等）。这在处理PDFs、网页或演示文稿等结构复杂的文档时会导致问题。
+ 查询不准确：由于缺乏对文档结构的理解，当用户提出与文档结构有关的问题（例如，“表3中哪一年的收益最高？”）时，传统的LLM往往无法准确回答。
+ 信息获取不全面：在处理结构复杂的文档时，仅仅依赖文本内容可能会导致信息获取不全面或不准确。

工作原理：

根据文档的结构信息，准确地回答用户提出的各种问题。例如，用户可以提出“请总结第5-7页的内容”或“表3中哪一年的收益最高”等问题，PDFTriage能够准确地提供答案。

+ 获取元数据：首先，该技术会生成文档的结构化元数据表达，包括文档各个部分（如段落、标题、表格等）的信息。
+ 选择相关内容：当用户提出一个问题时，该技术会根据元数据选择与问题最相关的文档部分（如特定页面、表格等）。比如，如果问题是“第5-7页的内容是什么？”，它会直接定位到这几页的内容。
+ 生成答案：最后，选定的文档部分和用户的问题会被LLM处理，以生成准确的答案。

用户反馈，PDFTriage生成的答案在多页任务（如结构问题和表格推理）中排名更高，而在一般文本任务（如分类和文本问题）中排名较低。然而，在所有问题类别中，PDFTriage都优于页面检索和块检索方法。

## 2.02. [使用GPT-4 Advanced Data Analysis（原Code Interpreter）功能驱动的交互式模拟人生游戏](https://github.com/EmbraceAGI/LifeReloaded)

游戏内容由 GPT4 实时生成，给您包罗万象，丰富多彩的真实人生体验。

## 2.03. [Octogen：为开发者打造的开源代码解释器](https://github.com/dbpunk-labs/octogen)

## 2.04. [llmchain: Rust + LLM](https://github.com/shafishlabs/llmchain-rs)

## 2.05. [Lace：用 Rust 写的概率交叉分类引擎，带有可选的 Python 接口](https://github.com/promised-ai/lace)

# 3. 模型

## 3.01. LLaMA 2 Long: 与GPT-4持平，上下文长度达3.2万token

和竞争对手相比，在指令微调MMLU (5-shot)等测试集上，表现超过ChatGPT。

在人类评估（human evaluation）上甚至优于10万token的Claude 2，这个话题还在Reddit上引发了讨论。

要知道，这些对比版本中，LLaMA 2 Long使用的最大版本也只有70B，远小于其他大模型。

论文介绍，LLaMA 2 Long使用了4000亿token语料加持下，只改了一个超参数，就实现了如上效果。

# 4. 技巧 / 教程

## 4.01. [思维链推理相关文献列表](https://github.com/zchuz/CoT-Reasoning-Survey) 

## 4.02. [提升 ChatGPT 翻译质量的 Prompt](https://weibo.com/1727858283/NlsDSpPaa)

如果让ChatGPT翻译两次，先直译一次，然后再基于第一次结果意译，整体翻译质量会上一个大台阶，而且经过第二次翻译后，“机翻”的痕迹已经不明显了，很难看出来这是机器翻译的结果。

当然你还可以让ChatGPT继续重写润色，但经过我的实验，对于 GPT-4 来说两次翻译已经是性价比很好的方式，既不用增加很多Token和时间成本，同时得到没有什么“机翻”痕迹的结果。

要多次翻译，并且要输出每一次结果，下一次翻译要基于前面的迭代；翻译文章的时候，如果太长需要手动把文章拆成了大小合适的块，逐批次翻译，最好每次只是编辑之前的内容，避免上下文超长。

Prompt 如下：

``` txt
你是一位精通简体中文的专业翻译，曾参与《纽约时报》和《经济学人》中文版的翻译工作，因此对于新闻和时事文章的翻译有深入的理解。我希望你能帮我将以下英文新闻段落翻译成中文，风格与上述杂志的中文版相似。

规则：
- 翻译时要准确传达新闻事实和背景。
- 保留特定的英文术语或名字，并在其前后加上空格，例如："中 UN 文"。
- 分成两次翻译，并且打印每一次结果：
 
1. 根据新闻内容直译，不要遗漏任何信息
2. 根据第一次直译的结果重新意译，遵守原意的前提下让内容更通俗易懂，符合中文表达习惯

本条消息只需要回复OK，接下来的消息我将会给你发送完整内容，收到后请按照上面的规则打印两次翻译结果。
```

## 4.03. 让ChatGPT 校对翻译结果，检查有误遗漏呢？

我们可以将中文反向翻译回英文对比原始英文的方法来帮助校验翻译的结果是不是有明显遗漏或者添加了额外的信息。

参考Prompt：

``` txt
现在你是专业的翻译审查员，需要对于以下英文翻译成中文的结果进行审查，避免添加或遗漏重要信息导致重大的法律风险。

要求你使用以下步骤进行审查：

1. 忽略“原始英文”，将“中文翻译”反向翻译为英文（反向翻译英文），并打印结果，翻译时按照字面意思直译，尽可能保留原意。
2. 对比“原始英文”和“反向翻译英文”，列出有额外添加的内容或者遗漏的内容
3. 根据上面对比的结果，为“中文翻译”打分，指出其中不足之处

原始英文：
<原始英文>

中文翻译：
<中文翻译>

```

# 5. 资讯 / 观点

## 5.01. Anthropic/Claude 全面开放 API，与 GPT 的定价比较：

GPT4-32k context: 

+ $60/million input tokens 
+ $120/million output tokens

Claude2-100k context: 

+ $11.02/million input tokens 
+ $32.68/million output tokens

GPT3.5-Turbo-16k context:

+ $3/million input tokens
+ $4/million output tokens

Claude Instant-100k context: 

+ $1.63/million input tokens 
+ $5.51/million output tokens
